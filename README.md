# Russian_Binary_QA_with_synthetic_data

Одной из задач NLU (Natural Language Understanding) является генерация ответов на вопрос (англ. question-answering). Ее частным случаем является генерация ответа на общий вопрос и отличается от классического question-answering тем, что от модели не требуется в прямом смысле извлечь ответ из соответствующего текстового фрагмента, а просто предсказать один из двух возможных ответов – "да" или "нет". Таким образом, задача генерации ответа на общие вопросы сводится к бинарной классификации.

В данной работе для решения этой задачи планируется использовать предобученную языковую модель, а именно моноязычную модель для русского языка ruBERT (Kuratov & Arkhipov, 2019). Задача решается следующим образом: на вход модели подаются обе последовательности – вопрос и текстовых фрагмент, к которому задан этот вопрос, – которые соединены специальным токеном [SEP]. Обе последовательности преобразуются в векторные представления или эмбеддинги (англ. embedding). Поскольку данная задача представляет собой текстовую классификацию, то на выходной слой подается эмбеддинг [CLS]. После применяется softmax и в результате каждая такая пара (вопрос-текстовый фрагмент) относится к одному из заданных классов, т.е. модель либо предсказывает ответ "да", либо "нет". 

На данный момент единственным датасетом общих вопросов на русском языке является DaNetQA (Glushkova et al., 2020), который включает в себя три набора данных – обучающую, валидационную и тестовую выборки, размеры которых составляют 1749, 821 и 805 триплетов вида "вопрос–текстовый фрагмент–ответ" соответственно. Размер обучающих данных довольно небольшой, поэтому достичь SOTA результата (Acc=91.5%) невозможно при тонкой настройке ruBERT на небольшом количестве эпох (в пределах 5). Возможным решением может стать генерация искуственных данных. Этот метод повышения качества вопросно-ответных моделей ранее уже предлагался в этой области, например в работах (Lewis et al. 2021) и (Bartolo et al., 2021). 

В данной работе генерация общих вопросов в русском осуществляется двумя способами: 
1. p-tuning ruGPT-3, поскольку для p-tuning-а в отличие от fine-tuning-а не требуется большого количества размеченных данных, а результаты сопоставимы. Для реализации используется библиотека ruPrompts
2. правиловая генерация вопросов с помощью синтаксических преобразований, т.е. преобразование невопросительного предложения в вопросительное, а затем перефразирование получившихся вопросов с помощью нейросетевого рерайтера, чтобы добиться бóльшей синонимии и лексического разнообразия

Гипотеза состоит в том, что генерация синтетических данных с помощью p-tuning позволит в результате получить качество предсказаний выше. Тем более, эти данные будут более близки к естественным примерам, поскольку вопрос будет задан ко всему текстовому фрагменту, а не просто к одному предложению, как в случае с rule-based подходом. 

После расширения обучающей выборки датасета решается непосредственно задача генерации ответа на вопрос, т.е. бинарная классификация. Поскольку в (Liu et al., 2021) было показано, что p-tuning дает результаты выше классической тонкой настройки, то в данной работе также помимо тонкой настройки применяется еще и p-tuning. Реализация p-tuning для класификации осуществляется с помощью библиотеки OpenPrompt. 

---
## Генерация общих вопросов с помощью p-tuning

Для реализации p-tuning для генерации была выбрана библиотека ([ruPrompts](https://github.com/ai-forever/ru-prompts)), в которой можно самостоятельно обучить подводку для выбранной NLP задачи. 
Шаблон подводки для генерации общих вопросов выглядит следующим образом: 
```<P*10>{answer}<P*10>{passage}<P*10>```

<b>NB</b> Вполне возможно, что и другое количество обучаемых токенов в подводке может также привести к хорошему качеству генерации, тем не менее, в данной работе экспериментальны путем было выбрано именно 30 обучаемых токенов. 

<ins>Гиперпараметры обучения:</ins>
* шагов обучения – 2500 
* скорость обучения – 0.1
* оптимизатор – AdamW
* warm-up на первых 500 шагах, затем постепенное понижение скорости обучения с помощью косинусного планировщика (англ. cosine scheduler)
* размер батча на обучении – 4
* размер батча на валидации – 2
* gradient_accumulation_steps – 4

<ins>Стратегии генерации для вопросов с ответом "да" и с ответом "нет" отличались:</ins>
1. Для генерации вопросов с ответом "да" использовался поиск по лучу (англ. beam search) с beam_size=3 и num_return_sequences=3. В качестве финального результата брался самый длинный вопрос. Также мы столкнулись с проблемой, которая заключалась в том, что при поиске по лучу вопросы почти всегда начинались с определенных слов, что приводило к достаточно однообразной структуре вопросов. Для того, чтобы это предотвратить, на половине примеров было сделано следующее: повторяющиеся токены были добавлены как `bad_words_ids`, поэтому при генерации они никогда не генерировались. Это позволило разнообразить вопросы в их структуре. Параметр температуры (англ. temperature) был выставлен 0.9 
2. Для генерации вопросов с ответом "нет" использовалось сэмплирование (англ. multinomial sampling), т.е. beam_size=1, do_sample=True, но num_return_sequences=3. Как и в случае с позитивными вопросами, из сгенерированных трех вариантов выбирался самый длинный. Параметр температуры (англ. temperature) был выставлен 0.9 

Всего было сгенерировано 5967 примеров, но в итоге в дальнейшей классификации использовалось чуть меньше, т.к. бралось меньше примеров с ответом "да", чтобы решить проблему сильно дисбаланса классов. 

Средняя перплексия (англ. perplexity) на сгенерированных данных – **14.2** 
