# P-tuning ruBERT под задачу генерации ответа на общий вопрос


Дополнительно обучение подводки или p-tuning было впервые предложенное в работе (Liu et al., 2021). В статье было показано, что метод позволяет достичь более высоких результатов для задачи генерации текста и задач на понимание текста, чем привычная тонкая настройка. Поэтому интересно сравнить результаты предсказаний ruBERT в задаче генерации ответа на вопрос после тонкой настройки и после дообучения подводки.  

Для реализации p-tuning-а для классификации была использвана библиотека OpenPrompt, которая позволяет самостоятельно задать шаблон подводки, вербализатор (англ. verbalizer), т.е. задать те выражения естественного языка, которым соответствует каждый класс, выбрать гиперпараметры обучения. 

Шаблон подводки для генерации ответа на общий вопрос в русском выглядит как: 

`{"placeholder": "text_a"} Вопрос: {"placeholder": "text_b"} {"soft"} {"soft"} {"soft"} Ответ: {"mask"} .`

где text_a – текстовый фрагмент, к которому задан вопрос,
    
    text_b – общий вопрос, 
    
    soft – обучаемый токен, в данном случае было выборано три обучаемых токена
    
    mask – метка класса (0 или 1)
    
Вербализатор выглядел следующим образом: 
`0 -> нет`, `1 -> да`

<ins>Гиперпараметры обучения:</ins> 
* скорость обучения – 1e-5
* оптимизатор – AdamW
* размер батча на обучении – 8
* размер батчка на валидации – 8
* функция потерь (англ. loss function) – кросс-энтропия (torch.nn.CrossEntropyLoss())
* L2-регуляризация (aka weight decay) – 0.01, но 0 для смещения (англ. bias) и нормализации слоев (англ. layer normalization)

---

<ins>Результаты p-tuning ruBERT на исходном датасете:</ins>
* **Accuracy на валидации** = 0.673
* **Accuracy на тесте** = 0.643

|   | **Positive questions**  | **Negative Questions** |
|:-------------:|:-------------:|:-------------:
|Precision| 0.62 |  0.82 |
|Recall| 0.9  | 0.44 |
|F-1| 0.74 |  0.57 |

---

<ins>Результаты p-tuning ruBERT на расширенном датасете (добавлены данные сгенерированные p-tuning-ом):</ins>
* **Accuracy на валидации** = 0.73
* **Accuracy на тесте** = 0.714

|   | **Positive questions**  | **Negative Questions** |
|:-------------:|:-------------:|:-------------:
|Precision| 0.67  | 0.84  |
|Recall| 0.9  | 0.56 |
|F-1|0.77  | 0.67  |

---

<ins>Результаты p-tuning ruBERT на расширенном датасете (добавлены данные сгенерированные rule-based алгоритмом):</ins>
* **Accuracy на валидации** = 0.705
* **Accuracy на тесте** = 0.671

|   | **Positive questions**  | **Negative Questions** |
|:-------------:|:-------------:|:-------------:
|Precision| 0.65  | 0.81  |
|Recall| 0.88  | 0.53 |
|F-1|0.75  | 0.64  |
